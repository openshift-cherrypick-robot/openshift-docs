// Module included in the following assemblies:
//
// * migration/migrating_3_4/migrating-applications-with-cam-3-4.adoc
// * migration/migrating_4_1_4/migrating-applications-with-cam-4-1-4.adoc
// * migration/migrating_4_2_4/migrating-applications-with-cam-4-2-4.adoc

[id='migration-creating-migration-plan-cam_{context}']
= Creating a migration plan in the {mtc-short} web console

You can create a migration plan in the {mtc-full} ({mtc-short}) web console.

You can use _direct image migration_ and _direct volume migration_ to migrate images or volumes directly from the source cluster to the target cluster. Direct migration improves performance significantly.

.Prerequisites

* You must add source and target clusters and a replication repository to the {mtc-short} web console.
* The clusters must have network access to each other.
* The clusters must have network access to the replication repository.
* The clusters must be able to communicate using OpenShift routes on port 443.
* The clusters must have no `Critical` conditions.
* The clusters must be in a `Ready` state.
* The migration plan name must not exceed 253 lower-case alphanumeric characters (`a-z, 0-9`) and must not contain spaces or underscores (`_`).
* PV `Move` copy method: The clusters must have network access to the remote volume.
* PV `Snapshot` copy method:
** The clusters must have the same cloud provider (AWS, GCP, or Azure).
** The clusters must be located in the same geographic region.
** The storage class must be the same on the source and target clusters.

* Direct image migration:
** The source cluster must have its internal registry exposed to external traffic.
** The exposed registry route of the source cluster must be added to the cluster configuration using the {mtc-short} web console or with the `exposedRegistryPath` parameter in the `MigCluster` CR manifest.

* Direct volume migration:
** The PVs to be migrated must be valid.
** The PVs must be in a `bound` state.
** The PV migration method must be `Copy` and the copy method must be `filesystem`.

.Procedure

. In the {mtc-short} web console, click *Migration plans*.
. Click *Add migration plan*.
. Enter the *Plan name* and click *Next*.
. Select a *Source cluster*.
. Select a *Target cluster*.
. Select a *Replication repository*.
. Select the projects to be migrated and click *Next*.
. Select a *Source cluster*, a *Target cluster*, and a *Repository*, and click *Next*.
. In the *Namespaces* screen, select the projects to be migrated and click *Next*.
. In the *Persistent volumes* screen, click a *Migration type* for each PV:

* The *Copy* option copies the data from the PV of a source cluster to the replication repository and then restores it on a newly created PV, with similar characteristics, in the target cluster.
* The *Move* option unmounts a remote volume, for example, NFS, from the source cluster, creates a PV resource on the target cluster pointing to the remote volume, and then mounts the remote volume on the target cluster. Applications running on the target cluster use the same remote volume that the source cluster was using. The remote volume must be accessible to the source and target clusters.

. Click *Next*.
. In the *Copy options* screen, select a *Copy method* for each PV:

* *Snapshot copy* backs up and restores data using the cloud provider's snapshot functionality. It is significantly faster than *Filesystem copy*.
* *Filesystem copy* backs up the files on the source cluster and restores them on the target cluster.

. You can select *Verify copy* to verify data migrated with *Filesystem copy*. Data is verified by generating a checksum for each source file and checking the checksum after restoration. Data verification significantly reduces performance.

. Select a *Target storage class*.
+
You can change the storage class of data migrated with *Filesystem copy*.
. Click *Next*.
. In the *Migration options* screen, the *Direct image migration* option is selected if you specified an exposed image registry route for the source cluster. The *Direct PV migration* option is selected if you are migrating data with  *Filesystem copy*.
+
The direct migration options copy images and files directly from the source cluster to the target cluster. This option is much faster than copying images and files from the source cluster to the replication repository and then from the replication repository to the target cluster.
. Click *Next*.
. In the *Hooks* screen, click *Add Hook* to add a hook to the migration plan.
. Enter the hook name.
. If your hook is an Ansible playbook, click *Browse* to upload the playbook and update the *Ansible runtime image* field if you are using a custom Ansible image.
. If your hook is not an Ansible playbook, click *Custom container image* and specify the image name and path.
. Click *Source cluster* or *Target cluster* on which the hook should run.
. Enter the *Service account name* and the *Service account namespace* of the cluster.
. Select the migration step when the hook should run:

* *PreBackup*: Before backup tasks are started on the source cluster
* *PostBackup*: After backup tasks are complete on the source cluster
* *PreRestore*: Before restore tasks are started on the target cluster
* *PostRestore*: After restore tasks are complete on the target cluster

. Click *Add Hook* and then click *Close*.
+
You can add up to four hooks to a single migration plan. Each hook runs during a different migration step.

. Click *Finish* and then click *Close*.
+
The migration plan is displayed in the *Migration plans* list.
